# Conda

## 指令

激活环境：就会进入base环境，之后跟其他python安装一致。

```python
conda activate 
```

退出环境：

```python
conda deactivate
```

# Nvidia

直接上官网搜和自己电脑相配的显卡驱动（一般都是推荐最新的）驱动的版本一般不做要求，都是适配的（不考虑其稳定性的情况下）,下载放在/home/zcc目录下（方便寻找，不能放在中文目录下面）

安装的前提是**禁用nouveau驱动**

```
lsmod | grep nouveau			//如果输入之后没有输出，就说明已经禁用了
```

1. 查看GPU的使用情况（也可以验证驱动是否安装成功）

   ```
   nvidia-smi
   ```

2. 可以看系统是否已经连上了nvidia，正常会出现3个文件

   ```
   ls /dev/nvidia*
   ```

3. 卸载nvidia指令：

   ```
   sudo apt-get remove --purge nvidia*		//通用的卸载方式
   ```

   ```
   sudo /usr/bin/nvidia-uninstall    //在cuda安装时候附带安装的，可以用此卸载
   ```

4. 进入图形页面： **Alt + ctrl +F7 **   

   进入命令行：**Alt + ctrl +F1** 	

5. 在安装nvidia的时候需要关闭图形化界面

   ```
   sudo service lightdm stop
   ```

   在安装完成后重新打开图形化界面

   ```
    sudo service lightdm start
   ```

6. 该文件对所有人均可执行

   ```
   sudo chmod a+x NVIDIA-Linux-x86_64-440.36.run
   ```

   运行安装程序

   ```
   sudo ./NVIDIA-Linux-x86_64-440.36.run --dkms --no-opengl-files //表示只安装驱动文件，不安装OpenGL文件，Ubuntu的内核本身也有OpenGL、且与GUI显示息息相关，一旦NVIDIA的驱动覆写了OpenGL，在GUI需要动态链接OpenGL库的时候就引起问题
   ```

   安装过程中，会出现问是否安装**dkms**，选yes；是否**32位兼容**，选yes；**x-org**，建议no

# Cuda

1. 该文件对所有人均可执行	

```
sudo chmod a+x  cuda_8.0.44_linux.run 
```

​	运行cuda安装程序

```
sudo ./cuda_8.0.44_linux.run
```

​	首先会出现	一个文档，按**回车**让进度条到100%

​	问是否接受——accept

​	问是否安装nvidia——如果已经安装好驱动了，就选择no；否则，选择yes

​	其余均默认为yes/默认路径

​	提示是否安装openGL——如果是双显的系统，就选no（默认是no）

2. 卸载cuda

   注意不能进入到该目录下卸载，而是要在外面运行该语句

   ```
    sudo /usr/local/cuda-8.0/bin/uninstall_cuda_8.0.pl
   ```

   

# RCNN（区域CNN）

利用深度学习进行目标检测的开山之作。

大致思想：采用**selective search**方法预先获得候选区域（是较可能为物体），之后仅在这些**候选区域**上面采用**CNN**提取特征并判断。

## 1. 目标检测的概念

给定图片，能够找到物体的位置，并且标注出物体的种类（location+classification）

### classification——图像识别

输入图片—> 输出物体类别	评估：准确率  

CNN已经能够将图中的物体类别识别出来

### location——定位

输入图片—>输出物体在图片的位置(x, y, w, h) 评估：评价函数：交并比IOU（黑红边框的交集/黑红边框的并集，0~1，1时，即红黑边框完全重合）<img src="markdown_pic\image-20191201215223834.png" alt="image-20191201215223834" style="zoom:67%;" />

## 2. 算法步骤

1. 候选区域生成：利用**selective search**方法对一张图片生成1000~2000个候选区域（region proposals）
2. 特征提取：对每个候选区域采用**CNN(卷积神经网络)提取特征**
3. 类别判断：特征送入每一类的**SVM分类器**中，判别是否属于该类
4. 位置精修：使用**回归器**精细修正候选框的位置

## 3. 数据集

训练集：一个较大的识别库：标定了图片中物体的类别（classification），1000w图像，1000类；一个较小的检测库：标定每个图像的类别和位置（classification&location），1w图像，20类。

先使用识别库预训练得到CNN，后用检测库调优参数，最后在检测库上进行测评。

## 4. 基础知识

### 1. selective search——选择性搜索	

目标检测中，这是常用的区域提议算法(region proposal algorithm，即使用一些方法将图像划分为小的区域)

**exhaustive search**：穷举法，遍历每个像素点，但是搜索范围很大，计算量大，也导致提议的区域很多，所以不适合使用

selective search的优势：捕捉不同尺度；多样化；快速计算

#### 1. Hierarchical Grouping Algorithm——分层分组算法

图像中**区域特征**比像素更有代表性

1. 计算所有**邻近区域**之间的**相似性**
2. **最相似**的区域被组合在一起
3. 计算合并区域和相邻区域的相似性
4. 重复2、3过程，直到整个图像变成一个地区

每次迭代产生的更大的区域并将其添加到区域提议列表中，从**小到大**创建从小的segments到大的segments的区域提案。

<img src="markdown_pic\image-20191202100123927.png" alt="image-20191202100123927" style="zoom:67%;" />

输入：图片（三通道）

输出：物体位置的可能结果L

算法原理：

1. 基于**图的图像分割**得到**初始分割区域**R = {r1, r2, ...,rn}
2. 初始化相似度集合 S=∅ 
3. 计算两两**相邻区域**之间的**相似度**，将其添加到相似度集合S中
4. 从集合S中找到相似度最大的两个区域ri和rj，将其合并为一个区域rt，从集合中删除ri和rj相关的相邻区域的相似度计算，再计算rt和相邻区域的相似度（即，和ri、rj的相邻区域），再将结果加入到相似度集合中。并且将新的区域加入到区域R中。重复操作，直到S=∅
5. 获取每个区域的边界框（bounding boxes）L，输出位置的可能结果

####  2. **Diversification Strategies** ——多元化策略

即在计算相似度的时候，考量的点是多样的，使得划分的完全。如下图，每个图都应该使用不同的划分策略，才能划分正确：（a）物体之间有层次性；（b）颜色可以为划分策略；（c）纹理区分；（d）物体有层次性

<img src="markdown_pic\image-20191202105746828.png" alt="image-20191202105746828" style="zoom: 67%;" />



1.  利用各种不同不变性的色彩空间：将原始色彩空间->八种色彩空间

   <img src="markdown_pic\image-20191202110935609.png" alt="image-20191202110935609" style="zoom: 50%;" />

2.  采用不同的相似性度量：

   - **颜色相似度衡量**（ 对各个通道计算颜色直方图，然后取各个对应bins的直方图最小值。 ）；
   - **纹理相似度衡量**（ 计算每个区域的快速sift特征 ）；
   - **优先合并小区域**（ 保证在图像每个位置都是多尺度的在合并 ）；
   - **形状重合度衡量**（ 合并后的区域的bounding box越小，其重合度越高 ） ；
   - **综合各种距离**（加权计算各种相似度衡量距离）

3.  改变起始区域

   因为是基于图的图像分割得到的初始分割区域，所以初始分割区域影响很大，所以通过多种参数选取以产生初始化图像分割，也是多样性的一种扩充

#### 给区域打分

上述步骤能得到很多区域，但是需要衡量每个**区域能作为目标的可能性是不同的**，**每个区域有不同的分数**，从而根据需要筛选候选区域。

（文章中）给予**最先合并的图片较大的权重**，最后完整的图片的权重为1，倒数第二为2以此类推。权重重合就给乘上一个随机数，相同区域多次出现就权重叠加。所有区域有不同的分数，根据需要筛选目标区域数目，选择分数最好的n个候选区域。

### 2. CNN经典模型——AlexNet

是一个深层卷积神经网络，在图像识别分类效果远超其他，推广了CNN。和普通的CNN模型来说：

1. 采用非线性激活函数：**ReLU**；
2. 防止过拟合的方法：**Dropout**，**数据扩充**；
3. **多GPU**实现；
4. **LRN归一化层**的使用

#### 1. 使用ReLU激活函数

sigmoid等非线性函数，易出现梯度弥散或梯度饱和问题（输入的值很小或是很大时，神经元的梯度接近于0；<u>反向传播时因为需要乘上一个Sigmoid导数，会造成梯度越来越小，导致网络变的很难学习</u> ）

ReLU函数

<img src="markdown_pic\image-20191202212716240.png" alt="image-20191202212716240" style="zoom:50%;" />

因为导数为1，**计算量减少**，**收敛速度加快**

#### 2. 数据扩充

提供数据的量增加，算法的准确率也会提高，能够避免过拟合，能够进一步增大、加深网络结构。但训练数据集的数量有限的情况下，对**已有的数据进行变换**而扩充数据集。

常用的有：水平翻转；随机裁剪；平移变换；颜色、光照变换。

AlexNet在训练时，用到了：

-  **随机裁剪**，对256×256的图片进行随机裁剪到224×224，然后进行水平翻转，相当于将样本数量增加了（（256-224）^2）×2=2048倍； 
-  测试的时候，对左上、右上、左下、右下、中间分别做了5次裁剪，然后翻转，共**10个裁剪**，之后对**结果求平均**；
-  **对RGB空间做PCA（主成分分析）**，然后对主成分做一个（0, 0.1）的高斯扰动，也就是对颜色、光照作变换， 结果使错误率又下降了1%。（主成分分析，用来数据降维，利用线性变换，将当前空间的数据投影到另外的一个空间中，使得数据维度降低）

#### 3. 重叠池化

一般的池化是不重叠的，池化窗口和步长相同，池化出现在**池化层**，用来压缩数据和参数量，以减少过拟合。一般都是选择区域平均或是区域最大

AlexNet使用的池化是可重叠的，即池化的窗口大于步长。且是**最大重叠池化**。池化重叠池化可以避免过拟合。

#### 4. 局部归一化（LRN）

归一化，即被激活的神经元抑制相邻的神经元，局部归一化即实现**局部抑制**。将值**归一化到某个范围**内，因为ReLU的值可能会很大。响应比较**大**的值变得**相对更大**，并抑制其他反馈较小的神经元 ，提高**网络的泛化能力**（ 因为一旦每批训练数据的分布各不相同(batch 梯度下降)，那么网络就要在每次迭代都去学习适应不同的分布，这样将会大大降低网络的训练速度 ）

公式如下：i表示第i个核在位置（x,y）运用激活函数ReLU后的输出；n是同一位置上临近的kernal map的数目；N是kernal的总数 

<img src="markdown_pic\image-20191202222704451.png" alt="image-20191202222704451" style="zoom:50%;" />

<img src="markdown_pic\image-20191202222639437.png" alt="image-20191202222639437" style="zoom:67%;" />

#### 5. Dropout

为了**防止过拟合**，通过**修改网络本身的结构**来实现。对于某层神经元，通过定义的概率将**神经元置为0**，即该神经元不参与前向和后向传播。但保证输入层和输出层神经元个数不变，其余均按照普通的学习方法进行。下一次迭代的时候，又**随机置0一批新的神经元**。——这使得每次迭代生成的神经网络均不同。

<img src="markdown_pic\image-20191202223250119.png" alt="image-20191202223250119" style="zoom:67%;" />

#### 6. 多GPU训练

**加快训练速度**，将一半的神经元放在一个GPU上，另一半放在另一个上。

#### 7. 逐层解析AlexNet网络结构

<img src="markdown_pic\image-20191202224306979.png" alt="image-20191202224306979" style="zoom:80%;" />

共8层，**前5层为卷积层**，**后3层为全连接层**。最后一个全连接层的输出传递给一个**1000路的softmax层**，对应1000个类标签的分布。

ps：softmax层，最后的分类和归一化， 将一些输入映射为**0-1之间的实数**，并且归一化保证**和为1**，因此多分类的**概率之和也刚好为1**，并且值较大的概率也大，较小的概率变小（但是仍有一定的概率）。针对分类问题，即该层是判断输入的图片到底对应的是1000类里面的哪个。

<img src="C:\Users\surface\AppData\Roaming\Typora\typora-user-images\image-20191203084449861.png" alt="image-20191203084449861" style="zoom:70%;" />

##### 1. 第一层（卷积层）

 步骤为：卷积-->ReLU-->池化-->归一化 

 输入的原始图像大小为224×224×3（RGB图像），在训练时会经过预处理变为227×227×3。

**卷积**：本层使用96个11×11×3的卷积核进行卷积计算，生成新的像素。卷积核沿图像按一定的步长往x、y轴方向移动计算卷积，然后生成新的特征图，其大小为：floor((img_size - filter_size)/stride) +1 = new_feture_size，这边步长为4。所以生成的是，  (227-11)/4+1=55，55×55×96大小的数据。

**ReLU**： 经过ReLU单元的处理，生成激活像素层（只是将每个值都变化了一下），尺寸仍为55×55×96

**池化**： 取3×3范围内像素值最大的，并且步长 < 核的尺寸（会产生重叠），能够压缩数据量。这边尺寸为3×3，步长为2，图像的尺寸为 (55-3)/2+1=27。所以数据规模变成 27×27×96。

**归一化**：尺寸为5×5，归一化后的像素规模不变，仍为27×27×96。

##### 2. 第二层（卷积层）

2个GPU下运行，步骤为： 卷积-->ReLU-->池化-->归一化 

输入的数据是第一层的27×27×96的像素层。特殊的，每个像素层的上下左右边缘都被填充了2个像素 ，即(27+2+2)×(27+2+2)×96（分成两组27×27×48的像素层放在两个不同GPU中进行运算 ）

**卷积**： 本层使用了256个5×5×48的卷积核，分成2组，每组为128个，分给两个GPU进行卷积运算，即128个5×5×48。这边步长为1， (31-5)/1+1=27 ，27×27×256

**ReLu**：激活像素层， 27×27×256

**池化**： 池化运算的尺寸为3×3，步长为2 ，(27-3)/2+1=13，所以池化后规模为13×13×256

**归一化**： 归一化运算的尺度为5×5，结果为13×13×256的像素层 

##### 3. 第三层（卷积层）

步骤为： 卷积-->ReLU 

 输入数据为第二层输出的2组13×13×128（即256）的像素层。特别的，每幅像素层的上下左右边缘都填充1个像素，填充后变为 (13+1+1)×(13+1+1)×128

**卷积**： 一共有384个3×3×256的卷积核（每个GPU有192个）， 步长是1个像素 ，(15-3)/1+1=13。**两个GPU有通过交叉的虚线连接，也就是说每个GPU要处理来自前一层的所有GPU的输入**。

**ReLU**：激活的像素层，尺寸仍为13×13×384。

##### 4. 第四层（卷积层）

步骤为： 卷积-->ReLU

输入数据为第三层输出的2组13×13×192的像素层，特别的，每幅像素层的上下左右边缘都填充1个像素， (13+1+1)×(13+1+1)×192  

**卷积**：每个卷积核的尺寸是3×3×192。 步长是1 ，(13+1+1-3)/1+1=13，2个GPU卷积后生成13×13×384的像素层。**第四层的GPU之间没有虚线连接，也即GPU之间没有通信**。

**ReLU**： 生成激活像素层，尺寸仍为2组13×13×192像素层。

##### 5. 第五层（卷积层）

步骤为： 卷积-->ReLU-->池化

输入为，第四层输出的2组13×13×192的像素层，  特别的， 填充后的尺寸变为 (13+1+1)×(13+1+1) 。

**卷积**： 每个GPU都有128个卷积核，每个卷积核的尺寸是3×3×192，即256个3×3×192，卷积的步长是1个像素。  (13+1+1-3)/1+1=13 ，2个GPU卷积后生成13×13×256的像素层。  

 **ReLU** ： 生成激活像素层，尺寸仍为2组13×13×128像素层，由两个GPU分别处理。

 **池化** ： 池化运算的尺寸为3×3，步长为2。 (13-3)/2+1=6， 规模为两组6×6×128的像素层数据 ，即6×6×256

##### 6. 第六层（全连接层）

步骤为： 卷积（全连接）-->ReLU-->Dropout

输入为：第五层的输出数据6×6×256，

**卷积**：有4096个6×6×256卷积核，卷积核中的每个系数只与特征图（输入）尺寸的一个像素值相乘——全连接层的来历。卷积后的像素层尺寸为4096×1×1  

**ReLU**：ReLU激活函数生成4096个值 

**Dropout**：Dropout运算，输出4096个结果值 

##### 7. 第七层（全连接层）

步骤为：卷积（全连接）-->ReLU-->Dropout

第六层输出的4096个值和第七层的4096个神经元进行全连接，然后经ReLU进行处理后生成4096个数据，再经过Dropout处理后输出4096个数据。 

##### 8. 第八层（全连接层）

步骤为：全连接

第七层输出的4096个数据与第八层的1000个神经元进行全连接，经过训练后输出1000个float型的值，这就是预测结果。 

### 3.  **有监督预训练**& **无监督预训练** 

 **无监督预训练**： 预训练阶段的样本不需要人工标注数据，所以就叫做无监督预训练。 

**有监督预训练**：也称为迁移学习，一个任务训练好的参数，拿到另外一个任务，作为神经网络的初始参数值，比直接采用随机初始化的方法，能够提高精度。eg，利用有大量标注好的人脸年龄分类的图片数据，训练了一个CNN，用于人脸的年龄识别。又有一个新任务是，人脸性别识别，那么可以利用已经训练好的年龄识别CNN模型，去掉最后一层，然后其它的网络层参数就直接复制过来，继续进行训练，让它输出性别。

现实是，图片分类标注好的训练数据非常多，但是物体检测的标注数据却很少，如何用少量的标注数据，训练高质量的模型，这就是文献最大的特点，这篇论文采用了迁移学习的思想：先用了ILSVRC2012这个训练数据库（这是一个图片分类训练数据库），先进行网络图片**分类训练**。 

### 4. SVM分类器

SVM是由分类超平面定义的**判别分类器**，即给定一组带标签的训练样本，算法能够输出最优的超平面对测试样本进行分类，即提供一条直线将正负样本进行分离。如下图会出现无数条符合的直线能够分割，如何选择最优的？

<img src="markdown_pic\image-20191204153016931.png" alt="image-20191204153016931" style="zoom:60%;" />

规则如下：如果一条直接离坐标点太近，就不是最优的。（∵其对噪声太敏感，不能正确推广）——所以，**目标是找到一条分割线里所有的样本越远越好。**

SVM算法：找到一个超平面，**它到离它最近的训练样本越远越好**。——最优分割超平面最大化训练样本边界。

ps：一些定义

**分割超平面**：将两个数据集分割开的**直线**

**超平面**：数据集是N维的，需要N-1维的某对象对数据进行分割。如上图就是2维的数据集，所以就是1维的直线进行分割。该对象就是超平面。

**间隔**：一个点到分割面的距离，即点相对于分割面的距离

**最大间隔**：数据集相对于分割超平面的最大距离

**支持向量**：离分割超平面最近的那些点

最优分类超平面只有**少数支持向量决定**，**问题具有稀疏性**。

<img src="C:\Users\surface\AppData\Roaming\Typora\typora-user-images\image-20191204154951006.png" alt="image-20191204154951006" style="zoom:50%;" />



## 5. 各阶段详解

总体思路：对每个输入图片产生**2000个不分种类**的候选区域 -> 使用CNN提取每个候选区域**固定长度的特征向量**（4096维）-> 对取出的特征向量使用**特定种类的线性SVM**进行分类。即找出候选框 -> 利用CNN提取特征向量 -> 利用SVM进行特征向量分类。

### 1. 候选框搜索阶段

用selective search方法找出2000个候选框

候选框为矩形，但是大小不同，而CNN要求输入图片的大小是固定的，所以要对每个候选框进行缩放。如图(A)

各向异性缩放——不管照片是否扭曲，直接将像素值修改到指定大小。如图(D)

各向同性缩放——先扩充后裁剪（在原始图片中扩展候选区域为正方形，然后再裁剪到指定大小），如图(B)；先裁剪后扩充（先将候选区域从图片中裁剪出来，然后用固定的颜色填充剩下的区域，背景颜色是候选框像素颜色均值），如图(C)。

此外还有一个padding处理。1，3行为padding=0的结果；2，4为padding=16的结果（即，还加了原图的边界框）

<img src="markdown_pic\image-20191202204106300.png" alt="image-20191202204106300" style="zoom:67%;" />

根据结果分析——各向异性缩放+padding=16效果最好

### 2. CNN特征提取阶段⭐

#### 1.算法实现

##### a、 网络结构设计阶段

可以选择AlexNet（精确度低些，计算量小）；VGG16（精度高，但是计算量大）。

这边使用的AlexNet是5个卷积层，2个全连接层。

<img src="markdown_pic\image-20191203160631501.png" alt="image-20191203160631501" style="zoom:50%;" />

 通过这个网络训练完毕后，最后提取特征每个输入候选框图片都能得到一个4096维的特征向量。 

##### b、 网络有监督预训练阶段

由于物体检测中，物体标签的训练参数较少，直接采用随机初始化CNN（AlexNet网络）的参数，当前的训练数据量是不够的。所以这里采用的是**有监督的预训练**。所以本文是直接**使用AlexNet网络，初始化参数也是AlexNet中的**，再进行**fine-tuning训练**。 网络优化求解时采用随机梯度下降法，学习率大小为0.001。

##### c、fine-tuning阶段

利用selective search获得的候选框+预训练的CNN模型进行fine-tuning训练。把CNN模型的最后一层替换掉，替换成N+1输出的神经元（1：为背景），然后这层直接采用参数随机初始化的方法。采用**随机梯度下降法（SGD）**，学习率大小为0.001，batch_size = 128，其中32个为**正样本**，96为**负样本**。

ps：**正负样本**：在selective search阶段，一张图片得到了2000个候选框，人工标注的一张图片就只有正确的bounding box。所以在预处理阶段，我们需要对**每个候选框的bounding box打标签**——正样本or负样本。根据IOU，若候选框和人工标注的框的**IOU < 0.5 为负样本**（背景），否则为正样本（标注成对应的物体类别）。

ps：**SGD只用一个训练数据来更新参数**，而不用全部的训练数据；GD是全部数据

ps：如果不针对特定任务进行fine-tuning，而是把**CNN当做特征提取器**，卷积层所学到的特征其实就是基础的共享特征提取层，就类似于SIFT算法一样，可以**用于提取各种图片的特征**，而**f6、f7所学习到的特征是用于针对特定任务的特征**。eg：对于人脸性别识别来说，一个CNN模型前面的卷积层所学习到的特征就类似于学习人脸共性特征，然后全连接层所学习的特征就是针对性别分类的特征了

### 3. SVM训练和测试阶段

**训练阶段**：是**二分类问题**，由于检测集上面有20类（加上背景就是21类），所以就有**21个SVM组合**而成：第一个SVM的输出是属于分类1的概率，以此类推，根据这21个SVM的结果进行排序，哪个输出最大候选区域就是哪一类。

首先**确定正负样本**。由于候选区可能只包含物体的部分，所以需界定正负样本：测试发现，IOU=0.3时效果最好，即**IOU > 0.3为正样本，<0.3为负样本**。

在CNN f7层的特征被提取出来后，可以**为每个物体类训练一个SVM分类器**。候选框有2000个时，f7出来的就有2000×4096个特征向量矩阵，那么该矩阵和SVM权值矩阵4096×21(类别数)点乘，就可以得到结果了。

**位置精修**：**回归器**。对于每一类目标，使用一个线性脊回归器进行精修。正则项λ = 10000， 输入为深度网络pool5层的4096维特征，输出为xy方向的缩放和平移 。训练样本：判定为本类的候选框中和真值重叠面积大于0.6的候选框。 

**测试阶段**： 使用selective search的方法在测试图片上提取2000个region propasals ，将每个region proposals归一化到227x227，然后再CNN中正向传播，将最后一层得到的特征提取出来。然后对于每一个类别，使用为这一类训练的SVM分类器对提取的特征向量进行打分，得到测试图片中对于所有region proposals的对于这一类的分数 。 再使用贪心的非极大值抑制（NMS）去除相交的多余的框。再对这些框进行canny边缘检测，就可以得到bounding-box(then B-BoxRegression)。  

（非极大值抑制（NMS）先计算出每一个bounding box的面积，然后根据score进行排序，把score最大的bounding box作为选定的框，计算其余bounding box与当前最大score与box的IoU，去除IoU大于设定的阈值的bounding box。然后重复上面的过程，直至候选bounding box为空，然后再将score小于一定阈值的选定框删除得到这一类的结果（然后继续进行下一个分类））

### 4. 训练过程

 RCNN分为4个阶段：Proposal阶段、特征提取阶段、分类阶段、回归阶段。这4个阶段都是相互独立训练的。 

特征提取器是AlexNet（已经有预训练的基础了），Proposal阶段对每张图片产生了2000个候选区域，把这些图片依照正负样本比例传递给特征提取器，特征提取器根据输出的分类结果与标签结果进行比对，完成特征提取器的训练。（这边的分类方法就是AlexNet中的方法，而没有引入SVM）

Proposal和特征提取器已经训练完毕后，将它们的结果fc6输入到分类器SVM中，SVM输出与标签结果比对，完成SVM的训练。

回归器的训练也和SVM类似 ，回归器取的是pool5的结果，用于位置精修

### 5. 缺陷

由于proposal阶段会产生2000个候选区域，每个候选区域独立提取特征，即每个图片都进行了2000次CNN。

# anchor目标检测

anchor box ：锚框，是固定的参考框

首先预设一组不同尺度不同位置的**固定参考框**，覆盖几乎所有位置和尺度，每个参考框负责检测与其交并比大于阈值 (训练预设值，常用0.5或0.7) 的目标，anchor技术将问题转换为"**这个固定参考框中有没有认识的目标，目标框偏离参考框多远**"，不再需要多尺度遍历滑窗，真正实现了又好又快



# mmdetection

## 训练

1. 默认需要GPU才能运行

2. 使用tool/train.py

3. 目前跑的是单GPU运行：

   指令：python tools/train.py configs/mask_rcnn_r50_fpn_1x.py --gpus 1 –work_dir workdirs 

   分别表示：训练文件，配置文件，GPU数目（默认为1），模型checkpoint 文件的输出目录（迭代过程中保留一次数据）

4. train.py的源码

## 测试

1. 默认需要GPU才能运行

2. 使用tool/test.py

3. 目前跑的是单GPU运行：

   指令：python tools/test.py configs/cascade_rcnn_r50_fpn_1x.py checkpoints/cascade_ecnn_r50_fpn_1x_20190501-3b6211ab.pth --gpus 1 --out out.pkl

   分别表示：测试文件，配置文件（和训练中的配置文件一样），训练结果文件（即训练的输出文件，在checkpoints目录下，如果是自己训练的结果，就放在了训练结果自定义的目录下面，后缀为.pth），GPU数目（默认为1），输出的文件名字

4. test.py的源码





 