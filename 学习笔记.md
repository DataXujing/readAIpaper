# Conda

## 指令

激活环境：就会进入base环境，之后跟其他python安装一致。

```python
conda activate name
```

退出环境：

```python
conda deactivate
```

创建新的python环境

```
conda create -n name python=3.6   //name是自定义的，并且可以指定创建的python环境版本
```

查看安装的所有环境：

```
conda info --envs				//列出所有的环境名字和路径
```

通过requirements.txt安装库

`pip install -r requirements.txt`

获得当前环境相关的库

`pip freeze >requirements.txt`(名字可以自定义)

通过.wheel安装库的时候，可能会存在多个版本（不仅仅是要确定python的版本，还有一些细节：cp27mu类似的，除问题的只会是python2版本的），所以需要确定是否满足，否则： <u>*** is not a supported wheel on this platform</u>

```
import pip

print(pip.pep425tags.get_supported())
```



# Nvidia

直接上官网搜和自己电脑相配的显卡驱动（一般都是推荐最新的）驱动的版本一般不做要求，都是适配的（不考虑其稳定性的情况下）,下载放在/home/zcc目录下（方便寻找，不能放在中文目录下面）

安装的前提是**禁用nouveau驱动**

```
lsmod | grep nouveau			//如果输入之后没有输出，就说明已经禁用了
```

1. 查看GPU的使用情况（也可以验证驱动是否安装成功）

   ```
   nvidia-smi
   ```

2. 可以看系统是否已经连上了nvidia，正常会出现3个文件

   ```
   ls /dev/nvidia*
   ```

3. 卸载nvidia指令：

   ```
   sudo apt-get remove --purge nvidia*		//通用的卸载方式
   ```

   ```
   sudo /usr/bin/nvidia-uninstall    //在cuda安装时候附带安装的，可以用此卸载
   ```

4. 进入图形页面： **Alt + ctrl +F7 **   

   进入命令行：**Alt + ctrl +F1** 	

5. 在安装nvidia的时候需要关闭图形化界面

   ```
   sudo service lightdm stop
   ```

   在安装完成后重新打开图形化界面

   ```
    sudo service lightdm start
   ```

6. 该文件对所有人均可执行

   ```
   sudo chmod a+x NVIDIA-Linux-x86_64-440.36.run
   ```

   运行安装程序

   ```
   sudo ./NVIDIA-Linux-x86_64-440.36.run --dkms --no-opengl-files //表示只安装驱动文件，不安装OpenGL文件，Ubuntu的内核本身也有OpenGL、且与GUI显示息息相关，一旦NVIDIA的驱动覆写了OpenGL，在GUI需要动态链接OpenGL库的时候就引起问题
   ```

   安装过程中，会出现问是否安装**dkms**，选yes；是否**32位兼容**，选yes；**x-org**，建议no

# Cuda

1. 该文件对所有人均可执行	

```
sudo chmod a+x  cuda_8.0.44_linux.run 
```

​	运行cuda安装程序

```
sudo ./cuda_8.0.44_linux.run
```

​	首先会出现	一个文档，按**回车**让进度条到100%

​	问是否接受——accept

​	问是否安装nvidia——如果已经安装好驱动了，就选择no；否则，选择yes

​	其余均默认为yes/默认路径

​	提示是否安装openGL——如果是双显的系统，就选no（默认是no）

2. 卸载cuda

   注意不能进入到该目录下卸载，而是要在外面运行该语句

   ```
    sudo /usr/local/cuda-8.0/bin/uninstall_cuda_8.0.pl
   ```

3. 尝试编译cuda提供的例子

   ```
   cd /home/zc/NVIDIA_CUDA-8.0_Samples		//进入例子目录
   make									//编译，编译时间有点久，并且需要gcc环境
   sudo apt-get install gcc				//如果编译出错一般就是gcc环境问题，安装之后再编译
   ```

    显示`Finished building CUDA samples`就是编译成功了

   运行生成的文件：

   ```
   cd /home/zc/NVIDIA_CUDA-8.0_Samples/bin/x86_64/linux/release	//在上面的基础上进入
   ./deviceQuery
   ```

    显示` Result = PASS `代表成功运行。

   查看系统和cuda的连接情况：

   ```
   ./bandwidthTest
   ```

   显示`Result =PASS`显示成功。

# 一、RCNN（区域CNN）

利用深度学习进行目标检测的开山之作。

大致思想：采用**selective search**方法预先获得候选区域（是较可能为物体），之后仅在这些**候选区域**上面采用**CNN**提取特征并判断。

## 1. 目标检测的概念

给定图片，能够找到物体的位置，并且标注出物体的种类（location+classification）

### classification——图像识别

输入图片—> 输出物体类别	评估：准确率  

CNN已经能够将图中的物体类别识别出来

### location——定位

输入图片—>输出物体在图片的位置(x, y, w, h) 评估：评价函数：交并比IOU（黑红边框的交集/黑红边框的并集，0~1，1时，即红黑边框完全重合）<img src="markdown_pic\image-20191201215223834.png" alt="image-20191201215223834" style="zoom:67%;" />

## 2. 算法步骤

1. 候选区域生成：利用**selective search**方法对一张图片生成1000~2000个候选区域（region proposals）
2. 特征提取：对每个候选区域采用**CNN(卷积神经网络)提取特征**
3. 类别判断：特征送入每一类的**SVM分类器**中，判别是否属于该类
4. 位置精修：使用**回归器**精细修正候选框的位置

## 3. 数据集

训练集：一个较大的识别库：标定了图片中物体的类别（classification），1000w图像，1000类；一个较小的检测库：标定每个图像的类别和位置（classification&location），1w图像，20类。

先使用识别库预训练得到CNN，后用检测库调优参数，最后在检测库上进行测评。

## 4. 基础知识

### 1. selective search——选择性搜索	

目标检测中，这是常用的区域提议算法(region proposal algorithm，即使用一些方法将图像划分为小的区域)

**exhaustive search**：穷举法，遍历每个像素点，但是搜索范围很大，计算量大，也导致提议的区域很多，所以不适合使用

selective search的优势：捕捉不同尺度；多样化；快速计算

但是selective search不是越多越好，更多的候选区会损失精度。召回率分析——召回率就是候选区域为真（IOU 大于阈值即为真）的窗口和Ground Truth的比值（被正确识别的正类别比例占所有正值的大小，eg：被识别的真值数/实际存在的真值个数）

recall =  正确识别的真值数/实际存在的真值个数 comsum(gt)/num_pos   在检测中，根据得到的score从大到小排序，给这些结果打标签，即判断gt，**IOU>阈值的，gt = 1；IOU<阈值的，gt = 0**

precision = 被正确识别的个数数/所有判定个数 comsum(gt)/index（已经预测了多少个正样本）。

AP就是PR（precision-recall）曲线下面积。

mAP：各类别AP的平均值。

<img src="markdown_pic\image-20191205151015489.png" alt="image-20191205151015489" style="zoom:80%;" />

#### 1. Hierarchical Grouping Algorithm——分层分组算法

图像中**区域特征**比像素更有代表性

1. 计算所有**邻近区域**之间的**相似性**
2. **最相似**的区域被组合在一起
3. 计算合并区域和相邻区域的相似性
4. 重复2、3过程，直到整个图像变成一个地区

每次迭代产生的更大的区域并将其添加到区域提议列表中，从**小到大**创建从小的segments到大的segments的区域提案。

<img src="markdown_pic\image-20191202100123927.png" alt="image-20191202100123927" style="zoom:67%;" />

输入：图片（三通道）

输出：物体位置的可能结果L

算法原理：

1. 基于**图的图像分割**得到**初始分割区域**R = {r1, r2, ...,rn}
2. 初始化相似度集合 S=∅ 
3. 计算两两**相邻区域**之间的**相似度**，将其添加到相似度集合S中
4. 从集合S中找到相似度最大的两个区域ri和rj，将其合并为一个区域rt，从集合中删除ri和rj相关的相邻区域的相似度计算，再计算rt和相邻区域的相似度（即，和ri、rj的相邻区域），再将结果加入到相似度集合中。并且将新的区域加入到区域R中。重复操作，直到S=∅
5. 获取每个区域的边界框（bounding boxes）L，输出位置的可能结果

####  2. **Diversification Strategies** ——多元化策略

即在计算相似度的时候，考量的点是多样的，使得划分的完全。如下图，每个图都应该使用不同的划分策略，才能划分正确：（a）物体之间有层次性；（b）颜色可以为划分策略；（c）纹理区分；（d）物体有层次性

<img src="markdown_pic\image-20191202105746828.png" alt="image-20191202105746828" style="zoom: 67%;" />



1.  利用各种不同不变性的色彩空间：将原始色彩空间->八种色彩空间

   <img src="markdown_pic\image-20191202110935609.png" alt="image-20191202110935609" style="zoom: 50%;" />

2.  采用不同的相似性度量：

   - **颜色相似度衡量**（ 对各个通道计算颜色直方图，然后取各个对应bins的直方图最小值。 ）；
   - **纹理相似度衡量**（ 计算每个区域的快速sift特征 ）；
   - **优先合并小区域**（ 保证在图像每个位置都是多尺度的在合并 ）；
   - **形状重合度衡量**（ 合并后的区域的bounding box越小，其重合度越高 ） ；
   - **综合各种距离**（加权计算各种相似度衡量距离）

3.  改变起始区域

   因为是基于图的图像分割得到的初始分割区域，所以初始分割区域影响很大，所以通过多种参数选取以产生初始化图像分割，也是多样性的一种扩充

#### 给区域打分

上述步骤能得到很多区域，但是需要衡量每个**区域能作为目标的可能性是不同的**，**每个区域有不同的分数**，从而根据需要筛选候选区域。

（文章中）给予**最先合并的图片较大的权重**，最后完整的图片的权重为1，倒数第二为2以此类推。权重重合就给乘上一个随机数，相同区域多次出现就权重叠加。所有区域有不同的分数，根据需要筛选目标区域数目，选择分数最好的n个候选区域。

### 2. CNN经典模型——AlexNet

是一个深层卷积神经网络，在图像识别分类效果远超其他，推广了CNN。和普通的CNN模型来说：

1. 采用非线性激活函数：**ReLU**；
2. 防止过拟合的方法：**Dropout**，**数据扩充**；
3. **多GPU**实现；
4. **LRN归一化层**的使用

#### 1. 使用ReLU激活函数

sigmoid等非线性函数，易出现梯度弥散或梯度饱和问题（输入的值很小或是很大时，神经元的梯度接近于0；<u>反向传播时因为需要乘上一个Sigmoid导数，会造成梯度越来越小，导致网络变的很难学习</u> ）

ReLU函数

<img src="markdown_pic\image-20191202212716240.png" alt="image-20191202212716240" style="zoom:50%;" />

因为导数为1，**计算量减少**，**收敛速度加快**

#### 2. 数据扩充

提供数据的量增加，算法的准确率也会提高，能够避免过拟合，能够进一步增大、加深网络结构。但训练数据集的数量有限的情况下，对**已有的数据进行变换**而扩充数据集。

常用的有：水平翻转；随机裁剪；平移变换；颜色、光照变换。

AlexNet在训练时，用到了：

-  **随机裁剪**，对256×256的图片进行随机裁剪到224×224，然后进行水平翻转，相当于将样本数量增加了（（256-224）^2）×2=2048倍； 
-  测试的时候，对左上、右上、左下、右下、中间分别做了5次裁剪，然后翻转，共**10个裁剪**，之后对**结果求平均**；
-  **对RGB空间做PCA（主成分分析）**，然后对主成分做一个（0, 0.1）的高斯扰动，也就是对颜色、光照作变换， 结果使错误率又下降了1%。（主成分分析，用来数据降维，利用线性变换，将当前空间的数据投影到另外的一个空间中，使得数据维度降低）

#### 3. 重叠池化

一般的池化是不重叠的，池化窗口和步长相同，池化出现在**池化层**，用来压缩数据和参数量，以减少过拟合。一般都是选择区域平均或是区域最大

AlexNet使用的池化是可重叠的，即池化的窗口大于步长。且是**最大重叠池化**。池化重叠池化可以避免过拟合。

#### 4. 局部归一化（LRN）

归一化，即被激活的神经元抑制相邻的神经元，局部归一化即实现**局部抑制**。将值**归一化到某个范围**内，因为ReLU的值可能会很大。响应比较**大**的值变得**相对更大**，并抑制其他反馈较小的神经元 ，提高**网络的泛化能力**（ 因为一旦每批训练数据的分布各不相同(batch 梯度下降)，那么网络就要在每次迭代都去学习适应不同的分布，这样将会大大降低网络的训练速度 ）

公式如下：i表示第i个核在位置（x,y）运用激活函数ReLU后的输出；n是同一位置上临近的kernal map的数目；N是kernal的总数 

<img src="markdown_pic\image-20191202222704451.png" alt="image-20191202222704451" style="zoom:50%;" />

<img src="markdown_pic\image-20191202222639437.png" alt="image-20191202222639437" style="zoom:67%;" />

#### 5. Dropout

为了**防止过拟合**，通过**修改网络本身的结构**来实现。对于某层神经元，通过定义的概率将**神经元置为0**，即该神经元不参与前向和后向传播。但保证输入层和输出层神经元个数不变，其余均按照普通的学习方法进行。下一次迭代的时候，又**随机置0一批新的神经元**。——这使得每次迭代生成的神经网络均不同。

<img src="markdown_pic\image-20191202223250119.png" alt="image-20191202223250119" style="zoom:67%;" />

#### 6. 多GPU训练

**加快训练速度**，将一半的神经元放在一个GPU上，另一半放在另一个上。

#### 7. 逐层解析AlexNet网络结构

<img src="markdown_pic\image-20191202224306979.png" alt="image-20191202224306979" style="zoom:80%;" />

共8层，**前5层为卷积层**，**后3层为全连接层**。最后一个全连接层的输出传递给一个**1000路的softmax层**，对应1000个类标签的分布。

ps：softmax层，最后的分类和归一化， 将一些输入映射为**0-1之间的实数**，并且归一化保证**和为1**，因此多分类的**概率之和也刚好为1**，并且值较大的概率也大，较小的概率变小（但是仍有一定的概率）。针对分类问题，即该层是判断输入的图片到底对应的是1000类里面的哪个。

##### 1. 第一层（卷积层）

 步骤为：卷积-->ReLU-->池化-->归一化 

 输入的原始图像大小为224×224×3（RGB图像），在训练时会经过预处理变为227×227×3。

**卷积**：本层使用96个11×11×3的卷积核进行卷积计算，生成新的像素。卷积核沿图像按一定的步长往x、y轴方向移动计算卷积，然后生成新的特征图，其大小为：floor((img_size - filter_size)/stride) +1 = new_feture_size，这边步长为4。所以生成的是，  (227-11)/4+1=55，55×55×96大小的数据。

**ReLU**： 经过ReLU单元的处理，生成激活像素层（只是将每个值都变化了一下），尺寸仍为55×55×96

**池化**： 取3×3范围内像素值最大的，并且步长 < 核的尺寸（会产生重叠），能够压缩数据量。这边尺寸为3×3，步长为2，图像的尺寸为 (55-3)/2+1=27。所以数据规模变成 27×27×96。

**归一化**：尺寸为5×5，归一化后的像素规模不变，仍为27×27×96。

##### 2. 第二层（卷积层）

2个GPU下运行，步骤为： 卷积-->ReLU-->池化-->归一化 

输入的数据是第一层的27×27×96的像素层。特殊的，每个像素层的上下左右边缘都被填充了2个像素 ，即(27+2+2)×(27+2+2)×96（分成两组27×27×48的像素层放在两个不同GPU中进行运算 ）

**卷积**： 本层使用了256个5×5×48的卷积核，分成2组，每组为128个，分给两个GPU进行卷积运算，即128个5×5×48。这边步长为1， (31-5)/1+1=27 ，27×27×256

**ReLu**：激活像素层， 27×27×256

**池化**： 池化运算的尺寸为3×3，步长为2 ，(27-3)/2+1=13，所以池化后规模为13×13×256

**归一化**： 归一化运算的尺度为5×5，结果为13×13×256的像素层 

##### 3. 第三层（卷积层）

步骤为： 卷积-->ReLU 

 输入数据为第二层输出的2组13×13×128（即256）的像素层。特别的，每幅像素层的上下左右边缘都填充1个像素，填充后变为 (13+1+1)×(13+1+1)×128

**卷积**： 一共有384个3×3×256的卷积核（每个GPU有192个）， 步长是1个像素 ，(15-3)/1+1=13。**两个GPU有通过交叉的虚线连接，也就是说每个GPU要处理来自前一层的所有GPU的输入**。

**ReLU**：激活的像素层，尺寸仍为13×13×384。

##### 4. 第四层（卷积层）

步骤为： 卷积-->ReLU

输入数据为第三层输出的2组13×13×192的像素层，特别的，每幅像素层的上下左右边缘都填充1个像素， (13+1+1)×(13+1+1)×192  

**卷积**：每个卷积核的尺寸是3×3×192。 步长是1 ，(13+1+1-3)/1+1=13，2个GPU卷积后生成13×13×384的像素层。**第四层的GPU之间没有虚线连接，也即GPU之间没有通信**。

**ReLU**： 生成激活像素层，尺寸仍为2组13×13×192像素层。

##### 5. 第五层（卷积层）

步骤为： 卷积-->ReLU-->池化

输入为，第四层输出的2组13×13×192的像素层，  特别的， 填充后的尺寸变为 (13+1+1)×(13+1+1) 。

**卷积**： 每个GPU都有128个卷积核，每个卷积核的尺寸是3×3×192，即256个3×3×192，卷积的步长是1个像素。  (13+1+1-3)/1+1=13 ，2个GPU卷积后生成13×13×256的像素层。  

 **ReLU** ： 生成激活像素层，尺寸仍为2组13×13×128像素层，由两个GPU分别处理。

 **池化** ： 池化运算的尺寸为3×3，步长为2。 (13-3)/2+1=6， 规模为两组6×6×128的像素层数据 ，即6×6×256

##### 6. 第六层（全连接层）

步骤为： 卷积（全连接）-->ReLU-->Dropout

输入为：第五层的输出数据6×6×256，

**卷积**：有4096个6×6×256卷积核，卷积核中的每个系数只与特征图（输入）尺寸的一个像素值相乘——全连接层的来历。卷积后的像素层尺寸为4096×1×1  

**ReLU**：ReLU激活函数生成4096个值 

**Dropout**：Dropout运算，输出4096个结果值 

##### 7. 第七层（全连接层）

步骤为：卷积（全连接）-->ReLU-->Dropout

第六层输出的4096个值和第七层的4096个神经元进行全连接，然后经ReLU进行处理后生成4096个数据，再经过Dropout处理后输出4096个数据。 

##### 8. 第八层（全连接层）

步骤为：全连接

第七层输出的4096个数据与第八层的1000个神经元进行全连接，经过训练后输出1000个float型的值，这就是预测结果。 

### 3.  **有监督预训练**& **无监督预训练** 

 **无监督预训练**： 预训练阶段的样本不需要人工标注数据，所以就叫做无监督预训练。 

**有监督预训练**：也称为迁移学习，一个任务训练好的参数，拿到另外一个任务，作为神经网络的初始参数值，比直接采用随机初始化的方法，能够提高精度。eg，利用有大量标注好的人脸年龄分类的图片数据，训练了一个CNN，用于人脸的年龄识别。又有一个新任务是，人脸性别识别，那么可以利用已经训练好的年龄识别CNN模型，去掉最后一层，然后其它的网络层参数就直接复制过来，继续进行训练，让它输出性别。

现实是，图片分类标注好的训练数据非常多，但是物体检测的标注数据却很少，如何用少量的标注数据，训练高质量的模型，这就是文献最大的特点，这篇论文采用了迁移学习的思想：先用了ILSVRC2012这个训练数据库（这是一个图片分类训练数据库），先进行网络图片**分类训练**。 

### 4. SVM分类器

SVM是由分类超平面定义的**判别分类器**，即给定一组带标签的训练样本，算法能够输出最优的超平面对测试样本进行分类，即提供一条直线将正负样本进行分离。如下图会出现无数条符合的直线能够分割，如何选择最优的？

<img src="markdown_pic\image-20191204153016931.png" alt="image-20191204153016931" style="zoom:60%;" />

规则如下：如果一条直接离坐标点太近，就不是最优的。（∵其对噪声太敏感，不能正确推广）——所以，**目标是找到一条分割线里所有的样本越远越好。**

SVM算法：找到一个超平面，**它到离它最近的训练样本越远越好**。——最优分割超平面最大化训练样本边界。

ps：一些定义

**分割超平面**：将两个数据集分割开的**直线**

**超平面**：数据集是N维的，需要N-1维的某对象对数据进行分割。如上图就是2维的数据集，所以就是1维的直线进行分割。该对象就是超平面。

**间隔**：一个点到分割面的距离，即点相对于分割面的距离

**最大间隔**：数据集相对于分割超平面的最大距离

**支持向量**：离分割超平面最近的那些点

最优分类超平面只有**少数支持向量决定**，**问题具有稀疏性**。

<img src="markdown_pic\image-20191204154951006.png" alt="image-20191204154951006" style="zoom:50%;" />



## 5. 各阶段详解

总体思路：对每个输入图片产生**2000个不分种类**的候选区域 -> 使用CNN提取每个候选区域**固定长度的特征向量**（4096维）-> 对取出的特征向量使用**特定种类的线性SVM**进行分类。即找出候选框 -> 利用CNN提取特征向量 -> 利用SVM进行特征向量分类。

### 1. 候选框搜索阶段

用selective search方法找出2000个候选框

候选框为矩形，但是大小不同，而CNN要求输入图片的大小是固定的，所以要对每个候选框进行缩放。如图(A)

各向异性缩放——不管照片是否扭曲，直接将像素值修改到指定大小。如图(D)

各向同性缩放——先扩充后裁剪（在原始图片中扩展候选区域为正方形，然后再裁剪到指定大小），如图(B)；先裁剪后扩充（先将候选区域从图片中裁剪出来，然后用固定的颜色填充剩下的区域，背景颜色是候选框像素颜色均值），如图(C)。

此外还有一个padding处理。1，3行为padding=0的结果；2，4为padding=16的结果（即，还加了原图的边界框）

<img src="markdown_pic\image-20191202204106300.png" alt="image-20191202204106300" style="zoom:67%;" />

根据结果分析——各向异性缩放+padding=16效果最好

### 2. CNN特征提取阶段⭐

#### 1.算法实现

##### a、 网络结构设计阶段

可以选择AlexNet（精确度低些，计算量小）；VGG16（精度高，但是计算量大）。

这边使用的AlexNet是5个卷积层，2个全连接层。

<img src="markdown_pic\image-20191203160631501.png" alt="image-20191203160631501" style="zoom:50%;" />

 通过这个网络训练完毕后，最后提取特征每个输入候选框图片都能得到一个4096维的特征向量。 

##### b、 网络有监督预训练阶段

由于物体检测中，物体标签的训练参数较少，直接采用随机初始化CNN（AlexNet网络）的参数，当前的训练数据量是不够的。所以这里采用的是**有监督的预训练**。所以本文是直接**使用AlexNet网络，初始化参数也是AlexNet中的**，再进行**fine-tuning训练**。 网络优化求解时采用随机梯度下降法，学习率大小为0.001。

##### c、fine-tuning阶段

利用selective search获得的候选框+预训练的CNN模型进行fine-tuning训练。把CNN模型的最后一层替换掉，替换成N+1输出的神经元（1：为背景），然后这层直接采用参数随机初始化的方法。采用**随机梯度下降法（SGD）**，学习率大小为0.001，batch_size = 128，其中32个为**正样本**，96为**负样本**。

ps：**正负样本**：在selective search阶段，一张图片得到了2000个候选框，人工标注的一张图片就只有正确的bounding box。所以在预处理阶段，我们需要对**每个候选框的bounding box打标签**——正样本or负样本。根据IOU，若候选框和人工标注的框的**IOU < 0.5 为负样本**（背景），否则为正样本（标注成对应的物体类别）。

ps：**SGD只用一个训练数据来更新参数**，而不用全部的训练数据；GD是全部数据

ps：如果不针对特定任务进行fine-tuning，而是把**CNN当做特征提取器**，卷积层所学到的特征其实就是基础的共享特征提取层，就类似于SIFT算法一样，可以**用于提取各种图片的特征**，而**f6、f7所学习到的特征是用于针对特定任务的特征**。eg：对于人脸性别识别来说，一个CNN模型前面的卷积层所学习到的特征就类似于学习人脸共性特征，然后全连接层所学习的特征就是针对性别分类的特征了

### 3. SVM训练和测试阶段

**训练阶段**：是**二分类问题**，由于检测集上面有20类（加上背景就是21类），所以就有**21个SVM组合**而成：第一个SVM的输出是属于分类1的概率，以此类推，根据这21个SVM的结果进行排序，哪个输出最大候选区域就是哪一类。

首先**确定正负样本**。由于候选区可能只包含物体的部分，所以需界定正负样本：测试发现，IOU=0.3时效果最好，即**IOU > 0.3为正样本，<0.3为负样本**。

在CNN f7层的特征被提取出来后，可以**为每个物体类训练一个SVM分类器**。候选框有2000个时，f7出来的就有2000×4096个特征向量矩阵，那么该矩阵和SVM权值矩阵4096×21(类别数)点乘，就可以得到结果了。

**位置精修**：**回归器**。对于每一类目标，使用一个线性脊回归器进行精修。正则项λ = 10000， 输入为深度网络pool5层的4096维特征，输出为xy方向的缩放和平移 。训练样本：判定为本类的候选框中和真值重叠面积大于0.6的候选框。 

**测试阶段**： 使用selective search的方法在测试图片上提取2000个region propasals ，将每个region proposals归一化到227x227，然后再CNN中正向传播，将最后一层得到的特征提取出来。然后对于每一个类别，使用为这一类训练的SVM分类器对提取的特征向量进行打分，得到测试图片中对于所有region proposals的对于这一类的分数 。 再使用贪心的非极大值抑制（NMS）去除相交的多余的框。再对这些框进行canny边缘检测，就可以得到bounding-box(然后再边界框回归，B-BoxRegression)。  

（非极大值抑制（NMS）先计算出每一个bounding box的面积，然后根据score进行排序，把score最大的bounding box作为选定的框，计算其余bounding box与当前最大score与box的IoU，去除IoU大于设定的阈值的bounding box。然后重复上面的过程，直至候选bounding box为空，然后再将score小于一定阈值的选定框删除得到这一类的结果（然后继续进行下一个分类））

### 4. 训练过程

 RCNN分为4个阶段：Proposal阶段、特征提取阶段、分类阶段、回归阶段。这4个阶段都是相互独立训练的。 

特征提取器是AlexNet（已经有预训练的基础了），Proposal阶段对每张图片产生了2000个候选区域，把这些图片依照正负样本比例传递给特征提取器，特征提取器根据输出的分类结果与标签结果进行比对，完成特征提取器的训练。（这边的分类方法就是AlexNet中的方法，而没有引入SVM）

Proposal和特征提取器已经训练完毕后，将它们的结果fc6输入到分类器SVM中，SVM输出与标签结果比对，完成SVM的训练。

回归器的训练也和SVM类似 ，回归器取的是pool5的结果，用于位置精修

### 5. 缺陷

由于proposal阶段会产生2000个候选区域，每个候选区域独立提取特征，即每个图片都进行了2000次CNN。

# 二、SPP网络（空间金字塔池）

 SPP（spatial ppyramid pool）网络是针对RCNN存在的问题进行改进生成的网络。

在RCNN中selective search生成的2000个候选区域大小是不固定的，所以在输入CNN网络时需要将其裁剪为指定大小——但是这可能**降低图像的精确度**（如果是裁剪的话，裁剪后的图片可能不能包含整个物体；如果是变形的话，目标大小会失真）

why CNN输入的需要指定大小的图片呢？

卷积层是不需要指定大小的，因为其是滑动框进行卷积，与大小没有关系。只是图片越大生成的特征图（feature maps）也越大。而在全连接层，由于卷积层生成的不同的特征图（feature map）展开成不同特征向量（feature vetor），则全连接层就需要对应不同的vetor与之操作——**全连接层是预定义好了卷积核，需要输入固定大小的特征图**。

所以SPP的做法是，在卷积层之后增加一个SPP层，**SPP层将特征图拉成固定长度的特征向量**，然后再输入到全连接层中。下图是CNN处理和SPP处理的不同过程。

<img src = "markdown_pic\image-20191205092027636.png" alt="image-20191205092027636">

### 1. 空间金字塔池化

<img src="markdown_pic\image-20191205094101512.png" alt="image-20191205094101512">

拿上图举例，黑色图片是5层卷积之后得到的特征图（feature map），**使用不同的大小块来提取特征**——例如，4x4，2x2， 1x1，将这三张网格放到特征图上，可以得到21个块（空间位，spatial bins）。**每个块提取一个特征，那么就得到了21维的特征向量**。不同大小的格子组合来池化就是SPP。空间金字塔最大池化，在21个块中，计算每个块的最大值。

<img src="markdown_pic\image-20191205094946641.png" alt="image-20191205094946641">

所以，SPP的特色是**多尺度特征提取出固定大小的特征向量**。

eg：conv5之后的特征图是13x13（axa），金字塔层是3x3（nxn），2x2，1x1（一共得到14个特征向量）。如何计算每个格子的大小呢（即，将网格放在特征图上时，每个格子覆盖的特征图的大小）

**windows_size = a/n向上取整**， **batch_size(stride) = a/n向下取整**。



<img src = "markdown_pic\image-20191205101450252.png" alt="image-20191205101450252" style="zoom:80%;" >

<img src="markdown_pic\image-20191205101605695.png" alt="image-20191205101605695" style="zoom:67%;" />

### 2. SPP-net和RCNN的对比

<img src="markdown_pic\image-20191205101909496.png" alt="image-20191205101909496" style="zoom:80%;" />

RCNN：SS操作得到~2000个候选框 --> 将2000个候选框均缩放到227x227，然后依次输入到CNN中，每个候选框均提取出一个特征向量，4096维 --> 将每个候选框的特征向量输入到SVM中，识别是哪一类。

计算量十分大，每个图片都要进行2000次左右的CNN，重叠区域会重复提取特征，提取特征操作冗余。

SSP：SS操作得到~2000个候选框 --> 对**整张**待检测的图片输入到CNN中进行特征提取，得到特征图 --> 在特征图中找到各个候选区域，对各个候选框进行金字塔空间池化，获得固定长度的特征向量 --> 将特征向量输入到SVM中进行分类识别。

每个图片只需要做1次的CNN，速度加快。

**原图中的候选框经过多次卷积后，候选框的位置相对于原图没有变化**。How 原图的候选框映射到卷积后的特征图上面?

<img src="markdown_pic\image-20191205103134765.png" alt="image-20191205103134765" style="zoom:80%;" />

# 三、Fast-RCNN

提出Fast-RCNN的原因，因为RCNN存在较多的问题：**RCNN训练分多步**：候选区域需要SS来得到~2000个候选区，需要fine-tuning一个预训练的CNN神经网络，对21个类别训练一个SVM分类器，训练一个回归器对bounding-box进行回归（4个步骤都是分开训练的）；**时间和内存消耗较大**：在训练SVM和回归器时需要将网络训练的结果作为输入。所以这些数据均需要保存下来，存取的时间也是消耗较大的；测试的速度也较慢。SPP-Net能解决重复卷积的问题（1次卷积），但是也需要训练SVM分类器和回归器，这些需要保存CNN得到的特征向量。——**Fast-RCNN减少了训练步骤，也不需要保存CNN得到的特征向量**。

## 1. ROI Pooling(感兴趣区域池)

根据输入的图像，将**候选区域映射到特征图的对应位置** --> 将映射后的区域（目标区域）划分为**相同大小的sections**(部分，sections数量和输入的维数一致) --> 对每个section做**最大池化**操作（最大池化就是找出自己section内最大值）。

能从不同大小的方框得到固定大小的特征图，优势：提高速度。

如下图分别为从CNN前五层训练得到的某个特征图，然后根据输入的候选框进行标注，再对候选框进行划分(2x2)

对每个section做最大池化就得到了2x2的输出特征向量。

这个是受SPP-Net的启发得到的。

<img src="markdown_pic\image-20191205132449905.png" alt="image-20191205132449905" style="zoom:80%;" />

<img src="markdown_pic\image-20191205132517189.png" alt="image-20191205132517189" style="zoom:67%;" />

## 2. Fast-RCNN的实现步骤

<img src="markdown_pic\image-20191205134307655.png" alt="image-20191205134307655" style="zoom:80%;" />

在第二步就是基础的CNN网络中的前5层卷积层，得到的时整个图片的特征图，将特征图放到**ROI Pooling**中进行池化，即将每个候选框在特征图中对应并且将其**均匀划分成mxn维大小**（和全连接层的输入有关系），每块都做**max pooling**，那么候选区域的特征向量提取出来了，并且向量维数均是一致的。

第五阶段的输出输入到两个**并行的全连接层**中（**多任务**）——创新点之一

**cls_score用于分类**，输出的是k+1维的数组，用来表示k类和背景的概率大小。

**bbox_predict用于调整候选区域的位置**，输出的是4x(K+1)的数组，表示当它是k类的时候，应该平移缩放的参数。

**loss_cls**：评估分类的代价

**loss_bbox**：评估检测框定位代价。

总代价为两者的加权和。

<img src="markdown_pic\image-20191205135805474.png" alt="image-20191205135805474" style="zoom:80%;" >

采用**SVD(奇异值分解)**，并用前t个特征值近似替代，对Fast-RCNN网络末尾并行的全连接层进行分解，减少计算的复杂度加快速度。

利用窗口得分分别对每类物体进行**非极大值抑制**剔除重叠建议框，得到每个类别中回归修正后的得分最高的窗口。

## 四、Faster-RCNN

大致思想：目标检测的四个步骤——**候选区域生成、特征提取、分类、位置精修被统一到一个深度学习网络框架内**，没有重复计算，提高了运行速度。faster-RCNN：**区域生成网络+fast-rcnn**，即用**区域生成网络替代SS方法**。

论文重点：如何**设计区域生成网络**，如何**训练**区域生成网络，如何让区域生成网络和fast-rcnn**共享一个特征提取网络**。

## 1. 整体流程

1. 对输入的图片进行裁剪，将裁剪好的图片**输入到预训练好的分类网络中**，获得该图片对应的特征图
2. 在特征图上的**每个锚点上取9个候选的ROI**(感兴趣区域)，3不同尺度、3不同长宽比，并根据相应的比例**映射到原始图像**中，将候选的ROI输入到RPN网络中，**RPN网络会通过softmax判断anchors的正负值**（区分ROI是前景or背景），同时对其进行**边界框回归修正anchors**获得精确的位置（即计算前景的ROI和真实目标之间的bounding box的偏差值，包括▲x、▲y、▲w、▲h）
3. 对这些不同大小的ROI进行**ROI Pooling操作**（就是将其映射为特定大小的特征图），输出固定大小的feature map，为了送入后续的全连接层判定目标类别（全连接层要求输入的大小是统一的）
4. 将其输入简单的检测网络中，利用1x1的卷积进行分类（区分不同的类别，N+1类，多余的一类是背景，用于删除不准确的ROI），同时进行BB回归（精确调整预测的ROI和GT的ROI之间的偏差值）获得准确的位置坐标。

如下图展示了fast-rcnn整个网络结构：将任意P×Q的图像缩放至固定大小M×N --> 将图片输入到卷积网络中：**13个conv层+13个relu层+4个pooling层** --> RPN网络经过3×3卷积，分别生成positive anchors和对应的边界框回归的偏移量，得到实际的候选框 --> 在ROI池化层利用候选框+特征图提取出固定大小的特征图 --> 送入全连接层和softmax网络中做分类

<img src= "markdown_pic\image-20191205193947160.png" alt="image-20191205193947160">

## 2. 细节详解

### 1. 卷积层

卷积层一共有三类层：卷积层、激励层、池化层：13个conv层+13个relu层+4个pooling层。

其中所有卷积层的核大小为kernel_size = 3×3，padding = 1，步数stride = 1

所有池化层：kernel_size = 2×2，padding = 0，stride = 2

可以注意到，在卷积层，所有卷积都扩了边(M+2)×(N+2)，再做3×3的卷积，得到的仍是M×N的大小——卷积层不改变输出矩阵的大小 (M+2-3)/1+1 = M

而池化层，(M- 2)/2+1 =M/2-1+1 = M/2，它使得输出的长宽都变为原来的一半。

那么M×N的矩阵经过卷积层固定的变成 ( M / 16 ) × ( N / 16 )

### 2. RPN网络

是Faster_rcnn的一个巨大的优势之处。

RPN网络实际分为2部分，上部分通过**softmax分类anchors获得positive和negative分类**

下部分计算anchors的**边界框回归的偏移量**。

最后的proposal层负责综合positive anchors和对应的边界框回归的偏移量**得到准确的候选框**，同时剔除太小和太大的候选框——至此，**目标定位（location）已经完成**。

#### 1. 获得候选的ROI

 锚点：**特征图上最小的单位点**，即经过五层的特征提取网络，输出为 ( M / 16 ) × ( N / 16 )的特征图，那么初始的图像和特征图之间存在对应关系，特征图上的每个点对应到原始图像的上面有16个像素间隔。

针对**每个锚点根据不同尺寸**（114，228，456）和**不同的长宽比**{1：1，1：2，2：1}多尺度检测的思想产生9个初始的BB，那么最终能产生**[( M / 16 ) × ( N / 16 )x9]个候选的ROI**，但是这些BB很不准确，需要边界框回归修正BB位置。如下图这些框就是锚框，BB也可称。一共有9个

<img src="markdown_pic\image-20191205163345017.png" alt="image-20191205163345017" style="zoom:80%;" />

下图是论文中的一幅图：最后一个卷积层输出有num_output = 256，就说明有256个特征图，即每个点都是256维的。（ps： 在conv5之后，做了rpn_conv/3x3卷积且num_output=256，相当于每个点又融合了周围3x3的空间信息，同时256-d不变 ）

对每个点都有9(k)个anchors，每个anchor都区分了positive和negative，所以每个点都有2k个分数，每个anchor都有(x,y,w,h)4个偏移量（准确来说是，▲x、▲y、▲w、▲h），所以是4k坐标值。（但是所有anchors太多，所以在合适的anchors上随机选择128个正样本和128个负样本进行训练）

<img src="markdown_pic\image-20191205203244063.png" alt="image-20191205203244063"  >

<img src="markdown_pic\image-20191205164632509.png" alt="image-20191205164632509" style="zoom: 67%;" />

ps：RPN一般用到的都是**多通道图像+多卷积核**做卷积，eg：输入为3通道的，同时卷积核有2个，那么对于每个卷积核，先对3个输入通道分别做卷积，再将**结果相加就是卷积的输出**。所以，对于某个卷积层，无论输入图像有多少通道，输出图像的通道数总是等于卷积核的数量。（如下图输出就是2个通道）.

若对多通道图像做1x1的卷积，就是将原本独立的每个通道**联通**在了一起。

<img src="markdown_pic\image-20191205191328835.png" alt="image-20191205191328835" style="zoom:80%;" />



#### 2. 二分类和定位

分类是区分是背景(negative) or 前景(positive)，所以是一个二分类。

<img src="markdown_pic\image-20191205204903377.png" alt="image-20191205204903377" style="zoom:67%;" />

利用softmax判定positive和negative，再判断之前，先做了1×1的卷积，num_output = 18，即该卷积输出的结果为**(M/16)×(N/16)×18**，正好可以**对应每个点都有9个anchors，并且包含了其正负的分数**，都可以保存在该矩阵中。在softmax前后均有reshape，是由于便于softmax分类，是源于caffe的实现形式。（？？？？）

RPN网络中，利用anchors和softmax初步提取出了positive anchors作为候选区域。

**boundingbox regression**：anchors虽然能够初步定位出边界框，但是不准确，所以需要对框进行修改使IOU变大。

(x, y, w, h): 表示中心点坐标和方框的宽和高。给定一个 anchor ![[公式]](https://www.zhihu.com/equation?tex=A%3D%28A_%7Bx%7D%2C+A_%7By%7D%2C+A_%7Bw%7D%2C+A_%7Bh%7D%29) 和 ![[公式]](https://www.zhihu.com/equation?tex=GT%3D%5BG_%7Bx%7D%2C+G_%7By%7D%2C+G_%7Bw%7D%2C+G_%7Bh%7D%5D) ，寻找一种变换F，使得 ![[公式]](https://www.zhihu.com/equation?tex=F%28A_%7Bx%7D%2C+A_%7By%7D%2C+A_%7Bw%7D%2C+A_%7Bh%7D%29%3D%28G_%7Bx%7D%5E%7B%27%7D%2C+G_%7By%7D%5E%7B%27%7D%2C+G_%7Bw%7D%5E%7B%27%7D%2C+G_%7Bh%7D%5E%7B%27%7D%29)， 

一般是：平移dx、dy和缩放dw、dh。当anchor和GT相差较小时，可以看成是线性变换——用线性回归建模来微调。

下半部分也有一个1×1的卷积，并且num_output = 36,那么卷积输出的图像(M/16)×(N/16)×36，正好对应了一个点9个anchors，一个anchor有[dx、dy、dw、dh]

#### 3. proposal层

该层综合positive anchors和[dx、dy、dw、dh]，，另外还有一个输入为im_info，还有一个参数为feat_stride=16，对应的是4次pooling导致图片大小变成原来的1/16，用来计算anchor的偏移量

im_info存储的是，从原始图像P×Q变换到M×N的所有缩放信息 imfo = [M, N, scale_factor]

该层的前向传播步骤：

1. 生成anchors，对所有anchors进行位置修正
2. 按照输入的positive softmax scores从大到小排序，提取最前的N个anchors，就是提取修正位置后的positive anchors，就是提取了修正位置之后的positive anchors
3. 超出图像边界的positive anchors均判定为边界
4. 剔除很小的positive anchors(width < 门限 or height < 门限)
5. 进行非极大值抑制（根据置信度进行排序 --> 置信度最高的边界框添加到输入列表中，并且将其从边界框列表中删除 --> 计算所有边界框的面积 --> 计算置信度最高的边界框和其他候选框的IOU --> 从边界框列表中删除IOU大于阈值的边界框 --> 重复操作，直到边界框列表为空）
6. 输出修正后的边界框坐标，proposal=[x1, y1, x2, y2] 

### 3. ROI Pooling

收集proposal，计算得到相同大小的特征图。

### 4. 分类

通过全连接层和softmax计算每个proposal具体属于哪一类，并且输出其概率。利用回归更加精确的获得目标检测框的位置。

### 5. 训练过程

是在已经训练好的模型上继续进行训练得到的。

主要思想是：

先独立训练RPN，然后用这个RPN的网络权重对Fast-RCNN网络进行初始化并且用之前RPN输出proposal作为此时Fast-RCNN的输入训练Fast-RCNN；用Fast-RCNN的网络参数去初始化RPN。之后不断迭代这个过程，即循环训练RPN、Fast-RCNN。

大体分6个步骤：（再多几次循环也没有性能提升了）

1.  在已经训练好的model上，训练RPN网络（独立训练的）
2.  利用步骤1中训练好的RPN网络，收集proposals
3.  第一次训练Fast RCNN网络（独立训练）
4.  第二次训练RPN网络（共享了所有公共卷积层）
5.  再次利用步骤4中训练好的RPN网络，收集proposals 
6.  第二次训练Fast RCNN网络（共享了卷积层+ Faster-rcnn特有的网络层）

<img src = "markdown_pic\image-20191205215332941.png" alt="image-20191205215332941" style="zoom:67%;" >

## 3. 总结

下图是RCNN， fast-rcnn，faster-rcnn网络构成的区别

<img src="markdown_pic\image-20191205154547838.png" alt="image-20191205154547838" style="zoom:80%;" >

# anchor目标检测

anchor box ：锚框，是固定的参考框

首先预设一组不同尺度不同位置的**固定参考框**，覆盖几乎所有位置和尺度，每个参考框负责检测与其交并比大于阈值 (训练预设值，常用0.5或0.7) 的目标，anchor技术将问题转换为"**这个固定参考框中有没有认识的目标，目标框偏离参考框多远**"，不再需要多尺度遍历滑窗，真正实现了又好又快



# mmdetection

## 数据集

```
mmdetection
├── mmdet
├── tools
├── configs
├── data
│   ├── coco
│   │   ├── annotations
│   │   ├── train2017
│   │   ├── val2017
│   │   ├── test2017
│   ├── VOCdevkit
│   │   ├── VOC2007
│   │   ├── VOC2012
```

官方指定的数据集为coco，要按照上面的格式自定义数据集。

## 训练

1. 默认需要GPU才能运行

2. 使用tool/train.py

3. 目前跑的是单GPU运行：

   指令：python tools/train.py configs/mask_rcnn_r50_fpn_1x.py --gpus 1 –work_dir workdirs 

   分别表示：训练文件，配置文件，GPU数目（默认为1），模型checkpoint 文件的输出目录（迭代过程中保留一次数据）

4. train.py的源码

   主要是两个参数：` parse_args() `和`main()`

   ` parse_args() `：获取命令行参数

   `main()`：函数主入口，先获得config文件的数据和输入参数处理，

   ​			然后调用`build_detector()`来创建模型，将config配置文件中的数据加载到建立的模型中去；

   ​			再调用` build_dataset() `来注册数据集，根据获得cfg中的data字典其中的train字段作为参数

   ​			最后调用`train_detector`开始训练模型

5. builder.py的源码

   mmdet/models/builder.py出现

   `build_detector()`中出现在builder.py中。

   cfg就是根据输入获得的神经网络的配置文件

   ```python
   model = build_detector(
           # 获得config文件的model配置数据，train的配置数据，test的配置数据
           cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)
   
   ##mmdet/models/builder.py
   def build_detector(cfg, train_cfg=None, test_cfg=None):    
   	return build(cfg, DETECTORS, dict(train_cfg=train_cfg, test_cfg=test_cfg))
   
   def build(cfg, registry, default_args=None):
       # isinstance判断变量是否是已知类型eg list，说明有多个模型那就一个一个构建模型 即[{},{},{}]
       if isinstance(cfg, list):
           modules = [_build_module(cfg_, registry, default_args) for cfg_ in cfg]
           return nn.Sequential(*modules)
       # 否则就说明是dict类型，说明只有一个模型，直接调用注册函数
       else:
           return _build_module(cfg, registry, default_args)
   ```

   `DETECTORS`是已经在registery注册过的网络，里面包含了所有已经注册过的detector网络。

   ```python
   def _build_module(cfg, registry, default_args):    
       # 判断模型参数数据是否为dict，并且是否存在type键，即 type='CascadeRCNN', 一定要存在网络类型的标志，以调用相应的神经网络。    
   	assert isinstance(cfg, dict) and 'type' in cfg    
   	assert isinstance(default_args, dict) or default_args is None    
   	# 复制model数据    
   	args = cfg.copy()    
   	# 获取神经网络类型名字 CascadeRCNN，并且从arg中移除该键值    
   	obj_type = args.pop('type')    
   	# 判断type是否是字符串类型    
   	if mmcv.is_str(obj_type):        
       	# 判断网络名字是否已经注册，输入的神经网络不在注册的类型中就报错        
       	if obj_type not in registry.module_dict:            
           	raise KeyError('{} is not in the {} registry'.format(                
               	obj_type, registry.name))    
           #通过网络名字，获得实际的类
           obj_type = registry.module_dict[obj_type]    
       elif not isinstance(obj_type, type):        
           raise TypeError('type must be a str or valid type, but got {}'.format(            			type(obj_type)))    
       # 如果有train和test的配置数据    
       if default_args is not None:        
           for name, value in default_args.items():            
               #将其放入args中            
               args.setdefault(name, value)    
   	# **args是将字典得到的各个元素unpack分别与形参匹配送入函数中，针对CascadeRCNN就是传入到cascade_rcnn的CascadeRCNN类中，初始化一个类，即新建一个类的实例——CascadeRCNN    
   	# 注意不包含type，因为已经pop出去了    
   	# 每个都正好有对应——num_stages，pretrained，backbone，neck，rpn_head，bbox_roi_extractor，bbox_head，train_cfg，test_cfg，其余CascadeRCNN类中有的但是配置文件中没有的就默认为None    
   	return obj_type(**args)
   ```

   这个函数也是创建`BACKBONES`、`NECKS`、`ROI_EXTRACTORS`、`SHARED_HEADS`、`HEADS`、`LOSSES`、`DETECTORS`的模型的核心函数。 

   **搭建模型的思路**是：

   - 首先创建一个名为DETECTORS的注册表，是一个registry对象，该注册表有属性name="detector"、_module_dict是一个字典，用来存放detector对象——包括类名和类对象eg: "cascade-rcnn": CascadeRCNN()；
   - 读取配置文件，是字典类型的，是网络的整体结构也包括初始化的参数，还有type类型，用来存放该网络的名字——通过type值去寻找对应的网络对象；
   - 通过配置文件实例化对应网络对象

   ​    eg: cfg.model -> build(cfg.model, registry, ...) --type in registry._module_dict--> obj = registry._module_dict[ type ] --> obj.__init(cfg.model, ....)

6. registry.py的源码

   mmdet/models/registry.py

   在build.py中会被用到

   类的实例化，Regisitry是一个类，传入一个字符串，该字符串为属性name的值， 并且创建_module_dict这个字典来存放字典数据。

   如下图，是实例化的几个类，其他函数可以直接调用，属性有_name和_ _module_dict(dict)

   ```python
   BACKBONES = Registry('backbone')
   NECKS = Registry('neck')
   ROI_EXTRACTORS = Registry('roi_extractor')
   HEADS = Registry('head')
   DETECTORS = Registry('detector')
   ```

   ```python
   def __init__(self, name):
   	self._name = name
   	self._module_dict = dict()
   
   def _register_module(self, module_class):
        """Register a module.
   
        Args:
        	module (:obj:`nn.Module`): Module to be registered.
        """
        if not issubclass(module_class, nn.Module):
        	raise TypeError(
           	'module must be a child of nn.Module, but got {}'.format(
            		module_class))
        # class自带的函数__name__，获得类名
        module_name = module_class.__name__
   
        # 判断传入的module是否在_module_dict已经注册了，如果没有注册就去注册
        if module_name in self._module_dict:
        	raise KeyError('{} is already registered in {}'.format(
           	module_name, self.name))
        self._module_dict[module_name] = module_class
       
   def register_module(self, cls):    
   	self._register_module(cls)    
   	return cls
   ```

   提供给每个网络使用，每个网络类定义之前都有` @DETECTORS.register_module `，这会主动调用上面的函数`register_module`，然后将类作为形参传入，然后调用`_register_module`，每个网络都会被注册——`self._module_dict[module_name] = module_class`将`classname: class`(类名string：类(object))项加入到_module_dict中

7. train_detector()函数

   出现在tools/train.py main()函数中的最后一个方法中，开始训练的函数

   ```python
   train_detector(    
   	model,    			//build_detector()中获得的对应网络实例化对象
   	train_dataset,    	//注册的数据集
   	cfg,    			//导入的配置文件的全部
   	distributed=distributed,   	//是否分布式 
   	validate=args.validate,    	//是否验证
   	logger=logger)				//log_level，就是训练时的状态输出
   ```

   train_detector(...)函数出现在apis/train.py中——判断是否是分布式训练or非分布式，我选择的是非分布式训练_non_dist_train(model, dataset, cfg, validate=validate)

   然后去调用该函数`_non_dist_train()`

   ```python
   def _non_dist_train(model, dataset, cfg, validate=False):    
   	# prepare data loaders    
       #导入数据，并且获得数据相关的配置    
   	data_loaders = [        
   		build_dataloader(            
   			dataset,            
   			cfg.data.imgs_per_gpu,            
   			cfg.data.workers_per_gpu,            
   			cfg.gpus,            
   			dist=False)    
   		]    
   		# put model on gpus    
   		model = MMDataParallel(model, device_ids=range(cfg.gpus)).cuda()    
   		# 更新和计算影响模型训练和模型输出的网咯参数，使其逼近或达到最优值，从而最小化损失函数，使用各参数的梯度值来最小化损失函数，最常用的一阶优化算法是梯度下降    
   		# build runner optimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)    
   		runner = Runner(model, batch_processor, cfg.optimizer, cfg.work_dir,                    cfg.log_level)  
           #hook类似插件，实现额外功能，但是不会修改代码主体，比如说能定义一个hook查看中间变量的梯度值
   		runner.register_training_hooks(cfg.lr_config, cfg.optimizer_config,                                   cfg.checkpoint_config, cfg.log_config)    
   		if cfg.resume_from:        
   			runner.resume(cfg.resume_from)    
   		elif cfg.load_from:        
   			runner.load_checkpoint(cfg.load_from)    
   			runner.run(data_loaders, cfg.workflow, cfg.total_epochs)
   ```

   Runner类就是mmcv自己构建的能够来更好的进行pytorch模型训练。

   runner类来操控安排训练的各个环节，我们只需要把定义好的模型结构、数据集等都丢给runner就能实现模型的训练。

## 测试

<img src="markdown_pic\image-20191211163421237.png" alt="image-20191211163421237" style="zoom:67%;" />

1. 默认需要GPU才能运行

2. 使用tool/test.py

3. 目前跑的是单GPU运行：

   指令：python tools/test.py configs/cascade_rcnn_r50_fpn_1x.py checkpoints/cascade_ecnn_r50_fpn_1x_20190501-3b6211ab.pth --gpus 1 --out out.pkl

   分别表示：测试文件，配置文件（和训练中的配置文件一样），训练结果文件（即训练的输出文件，在checkpoints目录下，如果是自己训练的结果，就放在了训练结果自定义的目录下面，后缀为.pth），--gpus GPU数目（默认为1），--out 输出的文件名字（后缀为.pkl），如果没有指定输入文件就不会保存结果

   此外可以加：--eval，参数可以有`proposal_fast`，`proposal`，`bbox`，`segm`，`keypoints`，用来评估结果效果

   ​					--show，在运行的过程中会将标记的图片显示——由于显卡垃圾，图片展示太浪费时间。

4. 运行测试之后会获得测试文件 out.pkl，如何使用？

   ```python
   import pickle
   file = open("out.pkl","rb")
   data = pickle.load(file)
   print(data)
   ```

4. test.py的源码









 